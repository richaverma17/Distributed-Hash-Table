\documentclass[12pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=0.8in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{fancyhdr}

% % Page setup
% \pagestyle{fancy}
% \fancyhf{}
% \rhead{\thepage}
% \lhead{Chord: Distributed Hash Table Implementation}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

% Code listing setup
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{gray!10},
    commentstyle=\color{green!50!black},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{gray},
    stringstyle=\color{red!70!black},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

% Title information
\title{\textbf{Chord: A Distributed Hash Table Implementation}}
\author{Nikhil Chacko, Richa Verma}
\date{}

\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract}
This project presents a complete implementation of the Chord protocol, a structured peer-to-peer Distributed Hash Table (DHT) designed to provide efficient and decentralized key lookup. 
Our implementation leverages gRPC for inter-node communication and incorporates advanced features including configurable replication for fault tolerance, persistent storage, and real-time visualization. The system achieves $O(\log N)$ lookup complexity through finger table-based routing and maintains correctness through periodic stabilization. We demonstrate a fully functional distributed key-value store capable of handling dynamic node joins, departures, and fail-stop failures while maintaining data availability and consistency.
\end{abstract}

\section{Introduction}

\subsection{Background}
Distributed Hash Tables (DHTs) have become fundamental building blocks in modern distributed systems, enabling scalable, decentralized data storage and retrieval without centralized coordination. DHTs are employed in peer-to-peer file sharing systems (BitTorrent), distributed storage systems (Amazon DynamoDB, Apache Cassandra), content delivery networks, and blockchain technologies.

\subsection{The Chord Protocol}
Chord, introduced by Stoica et al. in 2001 at MIT, is one of the most influential DHT protocols due to its elegant simplicity and provable efficiency. The protocol addresses three fundamental challenges in distributed systems:

\begin{enumerate}[itemsep=0pt]
    \item \textbf{Decentralization}: No single point of failure or coordination bottleneck
    \item \textbf{Scalability}: Efficient routing with logarithmic complexity
    \item \textbf{Dynamic Membership}: Graceful handling of nodes joining and leaving
\end{enumerate}

\subsection{Project Motivation}
This project was undertaken to gain hands-on experience with:
\begin{itemize}[itemsep=0pt]
    \item Consistent hashing and its applications in load distribution
    \item Peer-to-peer network architectures and decentralized coordination
    \item Fault tolerance mechanisms in dynamic, unreliable networks
    \item Production-grade RPC communication using gRPC and Protocol Buffers
    \item Implementation challenges in distributed routing algorithms
\end{itemize}

\section{Chord Protocol Theory}

\subsection{Consistent Hashing}
Chord employs consistent hashing to map both nodes and keys onto a circular identifier space ranging from $0$ to $2^m - 1$, where $m$ is the number of bits in the identifier. Each node and key is assigned an $m$-bit identifier using a cryptographic hash function (SHA-1 in our implementation).

\textbf{Key Assignment Rule}: A key $k$ is assigned to the first node whose identifier equals or follows $k$ in the identifier space. This node is called the \textit{successor} of $k$, denoted as $\text{successor}(k)$.

The circular nature ensures that the identifier space wraps around: the successor of the largest identifier is the smallest identifier.

\subsection{Node Identifiers and Ring Structure}
Each node's identifier is computed as:
\begin{equation}
    \text{node\_id} = \text{SHA-1}(\text{IP:Port}) \bmod 2^m
\end{equation}

Similarly, each key's identifier is:
\begin{equation}
    \text{key\_id} = \text{SHA-1}(\text{key\_string}) \bmod 2^m
\end{equation}

This creates a \textit{Chord ring} where nodes are positioned by their identifiers, and the clockwise successor relationship defines data responsibility.

\subsection{Routing with Finger Tables}
The core innovation of Chord is the \textit{finger table}, which enables $O(\log N)$ lookup routing. Each node $n$ maintains an $m$-entry finger table where the $i$-th entry contains:

\begin{equation}
    \text{finger}[i].\text{start} = (n + 2^{i-1}) \bmod 2^m
\end{equation}
\begin{equation}
    \text{finger}[i].\text{node} = \text{successor}(\text{finger}[i].\text{start})
\end{equation}

The finger table entries are exponentially spaced, allowing nodes to ``jump'' large distances across the ring during lookup, achieving logarithmic routing complexity.

\subsection{Lookup Algorithm}
To find the node responsible for key $k$, node $n$ executes:

\begin{algorithm}[H]
\caption{Chord Lookup Algorithm}
\begin{algorithmic}[1]
\Function{FindSuccessor}{$k$}
    \If{$k \in (n, \text{successor}]$}
        \State \Return successor
    \Else
        \State $n' \gets$ \Call{ClosestPrecedingNode}{$k$}
        \State \Return \Call{$n'$.FindSuccessor}{$k$}
    \EndIf
\EndFunction
\State
\Function{ClosestPrecedingNode}{$k$}
    \For{$i = m$ down to $1$}
        \If{finger[$i$] $\in (n, k)$}
            \State \Return finger[$i$]
        \EndIf
    \EndFor
    \State \Return $n$
\EndFunction
\end{algorithmic}
\end{algorithm}

The algorithm recursively forwards the query to the closest preceding node, converging to the responsible node in $O(\log N)$ hops.

\subsection{Stabilization Protocol}
To maintain correctness as nodes dynamically join and leave, Chord employs a periodic stabilization protocol:

\begin{enumerate}[itemsep=0pt]
    \item \textbf{Stabilize()}: Verifies and updates successor pointers
    \item \textbf{Notify()}: Informs potential predecessors of presence
    \item \textbf{FixFingers()}: Incrementally refreshes finger table entries
    \item \textbf{CheckPredecessor()}: Detects and removes failed predecessors
\end{enumerate}

These operations run periodically (typically every 1 second) on each node, ensuring eventual consistency of routing state.

\section{System Architecture}

\subsection{Overall Design}
Our implementation consists of several layered components that separate concerns and enable modularity. The architecture is organized as follows:

\begin{itemize}[itemsep=0pt]
    \item \textbf{Client Interface Layer}: CLI and Web UI for user interaction
    \item \textbf{gRPC Communication Layer}: Inter-node RPC communication
    \item \textbf{Chord Protocol Layer}: Core routing and lookup algorithms
    \item \textbf{Component Layers}: Finger Table, Stabilization, Storage
\end{itemize}

\subsection{Core Components}

\subsubsection{Chord Node (node.py)}
The \texttt{Node} class implements the complete Chord protocol. Each node:
\begin{itemize}[itemsep=0pt]
    \item Maintains a unique identifier based on its address
    \item Stores successor, predecessor, and finger table
    \item Implements lookup routing and key-value operations
    \item Runs periodic stabilization in a background thread
    \item Communicates with other nodes via gRPC
\end{itemize}

Key attributes include:
\begin{lstlisting}[language=Python]
class Node:
    id: int                    # Node position in ring
    address: str               # IP:Port
    successor: NodeInfo        # Next node clockwise
    predecessor: NodeInfo      # Previous node
    finger_table: FingerTable  # Routing information
    storage: Storage           # Key-value store
    replication_factor: int    # Number of replicas
\end{lstlisting}

\subsubsection{Finger Table (finger\_table.py)}
Implements the routing data structure with $m = 8$ bits (256-position ring):

\begin{lstlisting}[language=Python]
class FingerTable:
    def start(self, i: int) -> int:
        return (self.node_id + 2**i) % RING_SIZE
    
    def get_interval(self, i: int):
        # Returns [start, end) interval for finger
        start = self.start(i)
        if i + 1 < self.m:
            end = self.start(i + 1)
        else:
            end = self.node_id
        return (start, end)
\end{lstlisting}

\subsubsection{Storage Layer (storage.py)}
Provides persistent key-value storage using JSON serialization:
\begin{itemize}[itemsep=0pt]
    \item In-memory dictionary for fast access
    \item Automatic persistence to disk on writes
    \item Per-node storage files in data/ directory
    \item Thread-safe operations
\end{itemize}

\subsubsection{Utility Functions (utils.py)}
Defines critical system parameters and hashing:
\begin{lstlisting}[language=Python]
RING_BITS = 8
RING_SIZE = 2 ** RING_BITS  # 256 positions

def hash_key(key: str) -> int:
    sha1 = hashlib.sha1(key.encode('utf-8'))
    return int(sha1.hexdigest(), 16) % RING_SIZE
\end{lstlisting}

\subsubsection{gRPC Communication (chord.proto)}
Defines the service interface with key operations:
\begin{itemize}[itemsep=0pt]
    \item FindSuccessor: Locate responsible node for ID
    \item GetPredecessor/GetSuccessor: Query neighbors
    \item Notify: Update predecessor information
    \item Ping: Heartbeat for failure detection
    \item Get/Put/Delete: Key-value operations
\end{itemize}

\subsection{Web Visualization Server}
A FastAPI-based web server (server.py) provides:
\begin{itemize}[itemsep=0pt]
    \item Real-time visualization of the Chord ring
    \item Interactive controls for adding/removing nodes
    \item Key-value operation interface
    \item WebSocket-based live updates
    \item Activity logging and statistics
\end{itemize}

\section{Implementation Details}

\subsection{Node Lifecycle}

\subsubsection{Joining the Network}
When a node joins, it executes the following sequence:

\begin{algorithm}[H]
\caption{Node Join Protocol}
\begin{algorithmic}[1]
\Function{Join}{existing\_node\_address}
    \If{existing\_node\_address is None}
        \State // Create new ring
        \State successor $\gets$ self
        \State predecessor $\gets$ None
        \State Initialize all fingers to self
    \Else
        \State // Join existing ring
        \State successor $\gets$ existing\_node.FindSuccessor(self.id)
        \State predecessor $\gets$ None  // Set by stabilization
        \State Initialize finger table
        \State Notify successor about potential predecessor
    \EndIf
    \State Start stabilization thread
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Stabilization Loop}
Each node runs a background thread that periodically executes:

\begin{lstlisting}[language=Python]
def stabilization_loop():
    while running:
        stabilize()         # Verify successor
        fix_fingers()       # Update one finger entry
        check_predecessor() # Detect failed predecessor
        time.sleep(1.0)     # 1-second interval
\end{lstlisting}

\subsection{Key-Value Operations with Replication}

\subsubsection{PUT Operation}
To store a key-value pair with replication factor $R$:

\begin{algorithm}[H]
\caption{Replicated PUT Operation}
\begin{algorithmic}[1]
\Function{Put}{key, value}
    \State key\_hash $\gets$ hash(key)
    \State primary $\gets$ FindSuccessor(key\_hash)
    \State replicas $\gets$ [primary]
    \State current $\gets$ primary
    \For{$i = 1$ to $R-1$}
        \State current $\gets$ current.successor
        \State replicas.append(current)
    \EndFor
    \State success\_count $\gets$ 0
    \ForAll{node in replicas}
        \If{node.StorageWrite(key, value) succeeds}
            \State success\_count $\gets$ success\_count + 1
        \EndIf
    \EndFor
    \State \Return success\_count $\geq \lceil R/2 \rceil$
\EndFunction
\end{algorithmic}
\end{algorithm}

This provides:
\begin{itemize}[itemsep=0pt]
    \item \textbf{Fault Tolerance}: Data survives $R-1$ node failures
    \item \textbf{Load Distribution}: Writes distributed across successors
    \item \textbf{Quorum Consistency}: Majority of replicas must succeed
\end{itemize}

\subsubsection{GET Operation}
Retrieval attempts to read from replicas with fallback:

\begin{lstlisting}[language=Python]
def get(self, key: str):
    key_hash = hash_key(key)
    replica_nodes = self.get_replica_nodes(key_hash)
    
    for node in replica_nodes:
        try:
            value = node.storage.get(key)
            if value is not None:
                return value
        except Exception:
            continue
    
    return None
\end{lstlisting}

\subsubsection{DELETE Operation}
Ensures consistency by deleting from all replicas:

\begin{lstlisting}[language=Python]
def delete(self, key: str) -> bool:
    replica_nodes = self.get_replica_nodes(hash_key(key))
    found_on_any = False
    
    for node in replica_nodes:
        if node.storage.delete(key):
            found_on_any = True
    
    return found_on_any
\end{lstlisting}

\subsection{Range Checking on Circular Space}
A critical implementation detail is correctly handling circular range checks:

\begin{lstlisting}[language=Python]
def in_range(self, key, start, end, 
             inclusive_start=False, 
             inclusive_end=False):
    if start < end:
        # Normal case: no wraparound
        return start < key < end
    else:
        # Wraparound: key > start OR key < end
        return key > start or key < end
\end{lstlisting}

This ensures correct behavior across the ring boundary (e.g., checking if key 5 is between nodes 250 and 10).

\subsection{Failure Detection}
Nodes detect failures through heartbeat pinging:

\begin{lstlisting}[language=Python]
def check_predecessor(self):
    if self.predecessor:
        try:
            stub = self.create_stub(
                self.predecessor.address)
            stub.Ping(Empty(), timeout=2.0)
        except:
            # Predecessor failed, clear it
            self.predecessor = None
\end{lstlisting}

Failed nodes are automatically removed from routing state during stabilization.

\section{System Features}

\subsection{Replication and Fault Tolerance}
\textbf{Configurable Replication Factor}: Default 3x replication means each key is stored on three successive nodes. This provides:
\begin{itemize}[itemsep=0pt]
    \item Survival of up to 2 simultaneous node failures per key
    \item Higher read availability (can read from any replica)
    \item Improved load distribution for popular keys
\end{itemize}

\textbf{Chain Replication}: The primary node forwards writes to $R-1$ successors in sequence, ensuring ordered application of updates.

\subsection{Persistent Storage}
Each node maintains a JSON file in the data/ directory. Data persists across node restarts, enabling recovery after crashes.

\subsection{Real-Time Visualization}
The web interface displays:
\begin{itemize}[itemsep=0pt]
    \item Chord ring with nodes positioned by hash ID
    \item Replication relationships (green dashed lines)
    \item Key distribution across nodes
    \item Live statistics: active nodes, total keys, replication status
    \item Activity log with all operations timestamped
\end{itemize}

\subsection{Dynamic Membership}
Nodes can join and leave at any time:
\begin{itemize}[itemsep=0pt]
    \item \textbf{Join}: Node contacts any existing node, finds its position, initializes routing
    \item \textbf{Leave}: Node simply stops; successors and predecessors stabilize automatically
    \item \textbf{Crash}: Detected via ping timeout; routing state self-heals
\end{itemize}

\section{Performance Analysis}

\subsection{Lookup Complexity}
\textbf{Theoretical}: $O(\log N)$ hops where $N$ is the number of nodes.

\textbf{Practical}: With $m = 8$ bits and finger table size 8, a 256-node ring requires at most 8 hops. In practice:
\begin{itemize}[itemsep=0pt]
    \item Average case: 4-5 hops for medium-sized rings (50-100 nodes)
    \item Best case: 1 hop if target is immediate successor
    \item Worst case: $m$ hops in fully populated ring
\end{itemize}

\subsection{Storage Overhead}
\begin{itemize}[itemsep=0pt]
    \item \textbf{Replication Overhead}: Each key stored $R$ times ($R = 3$ default)
    \item \textbf{Routing State}: Each node stores $m$ finger entries (8 entries)
    \item \textbf{Memory per Node}: $O(K/N \cdot R + \log N)$ where $K$ is total keys
\end{itemize}

\subsection{Network Overhead}
\begin{itemize}[itemsep=0pt]
    \item \textbf{Stabilization Traffic}: Each node sends $O(1)$ messages per interval
    \item \textbf{Write Amplification}: Each PUT requires $R$ storage operations
    \item \textbf{Lookup Traffic}: $O(\log N)$ messages per lookup
\end{itemize}

\subsection{Convergence Time}
After network changes (join/leave):
\begin{itemize}[itemsep=0pt]
    \item Successor pointers stabilize in $O(1)$ rounds (1-2 seconds)
    \item Finger tables fully refresh in $O(m)$ rounds (8 seconds)
    \item System remains correct during convergence (may be suboptimal)
\end{itemize}

\section{Testing and Validation}

\subsection{Correctness Testing}
\textbf{Basic Operations}:
\begin{enumerate}[itemsep=0pt]
    \item Start 5 nodes, verify ring formation
    \item Store 20 keys, verify correct node placement
    \item Retrieve all keys from different nodes
    \item Delete keys, verify removal from all replicas
\end{enumerate}

\textbf{Replication Verification}:
\begin{enumerate}[itemsep=0pt]
    \item Store key on 5-node ring with $R=3$
    \item Verify key appears on exactly 3 consecutive nodes
    \item Kill primary node
    \item Verify key still retrievable from replicas
\end{enumerate}

\subsection{Fault Tolerance Testing}
\textbf{Single Node Failure}:
\begin{itemize}[itemsep=0pt]
    \item Store keys with 3x replication
    \item Terminate one node
    \item Verify all keys remain accessible
    \item Verify ring stabilizes within 2-3 seconds
\end{itemize}

\textbf{Cascading Failures}:
\begin{itemize}[itemsep=0pt]
    \item Start with 10 nodes
    \item Progressively kill nodes
    \item System remains functional until $R$ consecutive nodes fail
\end{itemize}

\subsection{Load Distribution}
We verified consistent hashing properties by:
\begin{enumerate}[itemsep=0pt]
    \item Adding 100 random keys to 8-node ring
    \item Measuring standard deviation of keys per node: $\sigma \approx 3.2$
    \item Adding/removing nodes, observing minimal key movement
\end{enumerate}

\section{Limitations}

\subsection{Security}
\begin{itemize}[itemsep=0pt]
    \item \textbf{No Authentication}: Any node can join the network
    \item \textbf{No Encryption}: Communication in plaintext
    \item \textbf{Byzantine Intolerance}: Cannot handle malicious nodes
\end{itemize}

\subsection{Consistency Model}
\begin{itemize}[itemsep=0pt]
    \item \textbf{Eventual Consistency}: No strong consistency guarantees
    \item \textbf{No Conflict Resolution}: Last-write-wins
    \item \textbf{Quorum-Based Writes}: Partial failures may leave inconsistent state
\end{itemize}

\subsection{Scalability Constraints}
\begin{itemize}[itemsep=0pt]
    \item \textbf{Fixed Ring Size}: 8-bit identifier space limits to 256 positions
    \item \textbf{No Virtual Nodes}: Load imbalance possible
    \item \textbf{Stabilization Overhead}: Increases linearly with network size
\end{itemize}

\subsection{Operational Limitations}
\begin{itemize}[itemsep=0pt]
    \item \textbf{In-Memory Storage}: Limited by RAM
    \item \textbf{Single Data Center}: No geographic replication
    \item \textbf{Manual Recovery}: No automated data migration after failures
\end{itemize}

\section{Future Work}

\subsection{Enhanced Fault Tolerance}
\begin{itemize}[itemsep=0pt]
    \item Implement successor lists (storing $r$ successors)
    \item Add virtual nodes for better load distribution
    \item Automatic key redistribution on membership changes
\end{itemize}

\subsection{Stronger Consistency}
\begin{itemize}[itemsep=0pt]
    \item Vector clocks for conflict detection
    \item Read repair mechanism
    \item Merkle trees for replica synchronization
    \item Configurable consistency levels
\end{itemize}

\subsection{Security Enhancements}
\begin{itemize}[itemsep=0pt]
    \item TLS encryption for gRPC channels
    \item Certificate-based node authentication
    \item Access control for operations
    \item Rate limiting to prevent DoS attacks
\end{itemize}

\subsection{Monitoring and Metrics}
\begin{itemize}[itemsep=0pt]
    \item Prometheus metrics export
    \item Grafana dashboards
    \item Performance profiling
    \item Alerting on failures and imbalance
\end{itemize}

\section{Conclusion}

This project successfully demonstrates a fully functional implementation of the Chord distributed hash table protocol with production-oriented enhancements. The system achieves its primary objectives:

\begin{enumerate}[itemsep=0pt]
    \item \textbf{Correctness}: Maintains ring invariants through stabilization, correctly routes lookups in $O(\log N)$ hops
    \item \textbf{Fault Tolerance}: Survives node failures through configurable replication
    \item \textbf{Scalability}: Supports dynamic membership with minimal disruption
    \item \textbf{Usability}: Provides intuitive interfaces for interaction
\end{enumerate}

The implementation validates the elegance and robustness of the Chord algorithm while highlighting practical challenges in distributed systems development. Key insights gained include:

\begin{itemize}[itemsep=0pt]
    \item Importance of circular range checking in ring-based protocols
    \item Trade-offs between consistency, availability, and partition tolerance
    \item Complexities of asynchronous distributed coordination
    \item Benefits of layered architecture
\end{itemize}

Beyond academic learning, this project provides a foundation for building more sophisticated distributed systems. The modular design enables straightforward extensions such as stronger consistency models, geographic replication, and security mechanisms.

\section*{Acknowledgments}
This implementation is based on the Chord protocol designed by Ion Stoica, Robert Morris, David Karger, M. Frans Kaashoek, and Hari Balakrishnan at MIT (2001).

\begin{thebibliography}{9}

\bibitem{chord}
Stoica, I., Morris, R., Karger, D., Kaashoek, M. F., \& Balakrishnan, H. (2001). 
\textit{Chord: A scalable peer-to-peer lookup service for internet applications}. 
ACM SIGCOMM Computer Communication Review, 31(4), 149-160.

\bibitem{dynamo}
DeCandia, G., et al. (2007). 
\textit{Dynamo: Amazon's highly available key-value store}. 
ACM SIGOPS Operating Systems Review, 41(6), 205-220.

\bibitem{cassandra}
Lakshman, A., \& Malik, P. (2010). 
\textit{Cassandra: a decentralized structured storage system}. 
ACM SIGOPS Operating Systems Review, 44(2), 35-40.

\bibitem{consistent-hashing}
Karger, D., et al. (1997). 
\textit{Consistent hashing and random trees}. 
ACM Symposium on Theory of Computing, 654-663.

\bibitem{cap}
Brewer, E. A. (2000). 
\textit{Towards robust distributed systems}. 
PODC, 7.

\end{thebibliography}

\end{document}
